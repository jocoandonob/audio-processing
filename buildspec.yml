version: 0.2

phases:
  install:
    runtime-versions:
      python: 3.9
  
  pre_build:
    commands:
      - echo "Logging in to Amazon ECR..."
      - aws ecr get-login-password --region $AWS_REGION | docker login --username AWS --password-stdin $AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com
      - REPOSITORY_URI=$AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com/track-audio-analysis
      - COMMIT_HASH=$(echo $CODEBUILD_RESOLVED_SOURCE_VERSION | cut -c 1-7)
      - IMAGE_TAG=${COMMIT_HASH:=latest}
      - echo "Creating ECR repository if it doesn't exist"
      - aws ecr describe-repositories --repository-names track-audio-analysis || aws ecr create-repository --repository-name track-audio-analysis
      
  build:
    commands:
      - echo "Build started on $(date)"
      - echo "Current directory:"
      - pwd
      - echo "Navigating to track-audio-analysis directory"
      - cd track-audio-analysis
      - echo "Directory contents:"
      - ls -la
      - echo "Building the Docker image..."
      - docker build -t $REPOSITORY_URI:latest .
      - docker tag $REPOSITORY_URI:latest $REPOSITORY_URI:$IMAGE_TAG
  
  post_build:
    commands:
      - echo "Build completed on $(date)"
      - echo "Pushing the Docker image..."
      - docker push $REPOSITORY_URI:latest
      - docker push $REPOSITORY_URI:$IMAGE_TAG
      - echo "Writing image definitions file..."
      - echo "{\"ImageURI\":\"$REPOSITORY_URI:$IMAGE_TAG\"}" > imageDefinitions.json
      - echo "Creating test script..."
      - |
        cat > test_job.py << 'EOF'
        import boto3
        import sys
        import time
        from datetime import datetime

        # Initialize SageMaker client
        sagemaker_client = boto3.client('sagemaker')

        # Generate a unique job name
        timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
        job_name = f"audio-analysis-test-{timestamp}"

        # Set variables
        input_uri = f"s3://{sys.argv[1]}/audio-analysis-input/"
        output_uri = f"s3://{sys.argv[1]}/audio-analysis-output/"
        role_arn = f"arn:aws:iam::{sys.argv[2]}:role/SageMakerAudioAnalysisRole"
        image_uri = f"{sys.argv[2]}.dkr.ecr.{sys.argv[3]}.amazonaws.com/track-audio-analysis:{sys.argv[4]}"

        print(f"Testing with:\nInput: {input_uri}\nOutput: {output_uri}\nRole: {role_arn}\nImage: {image_uri}")

        if len(sys.argv) > 5 and sys.argv[5].lower() == 'true':
            # Create processing job
            response = sagemaker_client.create_processing_job(
                ProcessingJobName=job_name,
                ProcessingResources={
                    'ClusterConfig': {
                        'InstanceCount': 1,
                        'InstanceType': 'ml.m5.xlarge',
                        'VolumeSizeInGB': 30
                    }
                },
                AppSpecification={
                    'ImageUri': image_uri,
                    'ContainerArguments': [
                        '--s3-input-uri', input_uri,
                        '--s3-output-uri', output_uri,
                        '--use-essentia', 'true'
                    ]
                },
                ProcessingInputs=[
                    {
                        'InputName': 'input-data',
                        'S3Input': {
                            'S3Uri': input_uri,
                            'LocalPath': '/opt/ml/processing/input/data',
                            'S3DataType': 'S3Prefix',
                            'S3InputMode': 'File',
                            'S3DataDistributionType': 'FullyReplicated'
                        }
                    }
                ],
                ProcessingOutputConfig={
                    'Outputs': [
                        {
                            'OutputName': 'analysis-output',
                            'S3Output': {
                                'S3Uri': output_uri,
                                'LocalPath': '/opt/ml/processing/output/analysis',
                                'S3UploadMode': 'EndOfJob'
                            }
                        }
                    ]
                },
                RoleArn=role_arn,
                Environment={
                    'ESSENTIA_TENSORFLOW_MODEL_PATH': '/opt/ml/processing/model/',
                    'ESSENTIA_TENSORFLOW_MODELS_DIR': '/opt/ml/processing/model/',
                    'ESSENTIA_EXTRACTORS_PATH': '/opt/ml/processing/model/discogs-effnet-bs64-1.pb',
                    'ESSENTIA_CLASSIFIER_GENRE_PATH': '/opt/ml/processing/model/genre_discogs400-discogs-effnet-1.pb',
                    'ESSENTIA_CLASSIFIER_MOOD_PATH': '/opt/ml/processing/model/mood_acoustic-discogs-effnet-1.pb'
                }
            )
            print(f"Processing job {job_name} created successfully")
        else:
            print("Test job creation skipped (set RUN_TEST=true to run)")
        EOF

      # Only run the test job if RUN_TEST is set to true
      - |
        if [ "${RUN_TEST}" = "true" ]; then
          echo "Running test job..."
          python test_job.py $S3_BUCKET_NAME $AWS_ACCOUNT_ID $AWS_REGION $IMAGE_TAG true
        else 
          echo "Skipping test job (set RUN_TEST=true to run)"
          python test_job.py $S3_BUCKET_NAME $AWS_ACCOUNT_ID $AWS_REGION $IMAGE_TAG false
        fi

artifacts:
  files:
    - imageDefinitions.json
    - test_job.py
    - appspec.yml
    - Dockerfile
    - buildspec.yml